{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Translation (MT) Experiments\n",
    "\n",
    "This notebook was executed on Google Colab using a T4 GPU. Please note that the document names in the outputs may differ from those in the GitHub repository. For consistency, refer to the file names in the code rather than those in the output, as the names were updated later for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ORQzOjtJJPn7",
    "outputId": "6337d4b7-666d-4510-a94a-da943298aeda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ctranslate2\n",
      "  Downloading ctranslate2-4.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting unbabel-comet\n",
      "  Downloading unbabel_comet-2.2.4-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
      "Collecting colorama (from sacrebleu)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (5.3.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from ctranslate2) (75.1.0)\n",
      "Collecting entmax<2.0,>=1.1 (from unbabel-comet)\n",
      "  Downloading entmax-1.3-py3-none-any.whl.metadata (348 bytes)\n",
      "Collecting jsonargparse==3.13.1 (from unbabel-comet)\n",
      "  Downloading jsonargparse-3.13.1-py3-none-any.whl.metadata (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (2.2.2)\n",
      "Requirement already satisfied: protobuf<5.0.0,>=4.24.4 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (4.25.5)\n",
      "Collecting pytorch-lightning<3.0.0,>=2.0.0 (from unbabel-comet)\n",
      "  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.5.4 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (1.13.1)\n",
      "Requirement already satisfied: sentencepiece<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (0.2.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (2.5.1+cu121)\n",
      "Collecting torchmetrics<0.11.0,>=0.10.2 (from unbabel-comet)\n",
      "  Downloading torchmetrics-0.10.3-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.1->unbabel-comet) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.1->unbabel-comet) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.1->unbabel-comet) (2024.2)\n",
      "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet)\n",
      "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.6.0->unbabel-comet) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (3.11.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.1->unbabel-comet) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->unbabel-comet) (3.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.18.3)\n",
      "Downloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ctranslate2-4.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unbabel_comet-2.2.4-py3-none-any.whl (96 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonargparse-3.13.1-py3-none-any.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.4/101.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading entmax-1.3-py3-none-any.whl (13 kB)\n",
      "Downloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchmetrics-0.10.3-py3-none-any.whl (529 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m529.7/529.7 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
      "Downloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: portalocker, lightning-utilities, jsonargparse, ctranslate2, colorama, sacrebleu, torchmetrics, entmax, pytorch-lightning, unbabel-comet\n",
      "Successfully installed colorama-0.4.6 ctranslate2-4.5.0 entmax-1.3 jsonargparse-3.13.1 lightning-utilities-0.11.9 portalocker-3.0.0 pytorch-lightning-2.4.0 sacrebleu-2.4.3 torchmetrics-0.10.3 unbabel-comet-2.2.4\n",
      "config.json: 100% 1.08k/1.08k [00:00<00:00, 7.27MB/s]\n",
      "2024-12-10 02:26:06.668543: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-10 02:26:06.705530: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-10 02:26:06.715724: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-10 02:26:06.739221: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-10 02:26:08.524535: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "model.safetensors: 100% 473M/473M [00:19<00:00, 24.1MB/s]\n",
      "generation_config.json: 100% 301/301 [00:00<00:00, 1.59MB/s]\n",
      "tokenizer_config.json: 100% 337/337 [00:00<00:00, 1.76MB/s]\n",
      "source.spm: 100% 801k/801k [00:00<00:00, 281MB/s]\n",
      "target.spm: 100% 837k/837k [00:00<00:00, 131MB/s]\n",
      "vocab.json: 100% 1.51M/1.51M [00:00<00:00, 6.64MB/s]\n",
      "special_tokens_map.json: 100% 65.0/65.0 [00:00<00:00, 305kB/s]\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install transformers sacrebleu ctranslate2 unbabel-comet\n",
    "!ct2-transformers-converter --model Helsinki-NLP/opus-mt-tc-big-en-lt --output_dir enlt_ctranslate2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "5XEq9h_232Ha",
    "outputId": "5be80387-bea8-43d8-9b67-767eef2f8409"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2024.9.11)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0aO1REX42czh"
   },
   "outputs": [],
   "source": [
    "import ctranslate2\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "vGy8JOX52TgC",
    "outputId": "602a73cd-23ea-4dae-84f3-042d682be721"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text: Labas, kaip sekasi?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "{\n",
      " \"name\": \"BLEU\",\n",
      " \"score\": 23.6,\n",
      " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
      " \"verbose_score\": \"60.0/25.0/16.7/12.5 (BP = 1.000 ratio = 1.000 hyp_len = 5 ref_len = 5)\",\n",
      " \"nrefs\": \"1\",\n",
      " \"case\": \"mixed\",\n",
      " \"eff\": \"no\",\n",
      " \"tok\": \"13a\",\n",
      " \"smooth\": \"exp\",\n",
      " \"version\": \"2.4.3\"\n",
      "},\n",
      "{\n",
      " \"name\": \"chrF2\",\n",
      " \"score\": 21.9,\n",
      " \"signature\": \"nrefs:1|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.4.3\",\n",
      " \"nrefs\": \"1\",\n",
      " \"case\": \"mixed\",\n",
      " \"eff\": \"yes\",\n",
      " \"nc\": \"6\",\n",
      " \"nw\": \"0\",\n",
      " \"space\": \"no\",\n",
      " \"version\": \"2.4.3\"\n",
      "},\n",
      "{\n",
      " \"name\": \"TER\",\n",
      " \"score\": 66.7,\n",
      " \"signature\": \"nrefs:1|case:lc|tok:tercom|norm:no|punct:yes|asian:no|version:2.4.3\",\n",
      " \"nrefs\": \"1\",\n",
      " \"case\": \"lc\",\n",
      " \"tok\": \"tercom\",\n",
      " \"norm\": \"no\",\n",
      " \"punct\": \"yes\",\n",
      " \"asian\": \"no\",\n",
      " \"version\": \"2.4.3\"\n",
      "}\n",
      "]\n",
      "\u001b[0m2024-12-10 02:41:16.474286: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-10 02:41:16.493963: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-10 02:41:16.499814: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-10 02:41:17.674816: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Seed set to 1\n",
      "Fetching 5 files: 100% 5/5 [00:00<00:00, 59409.41it/s]\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/371e9839ca4e213dde891b066cf3080f75ec7e72/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Predicting DataLoader 0: 100% 1/1 [00:01<00:00,  1.51s/it]\n",
      "wmt23_example-sys.en-lt.en\tSegment 0\tscore: 0.9309\n",
      "wmt23_example-sys.en-lt.en\tscore: 0.9309\n"
     ]
    }
   ],
   "source": [
    "# Figuring out the inner workings\n",
    "\n",
    "src_lang = \"en\"\n",
    "tgt_lang = \"lt\"\n",
    "\n",
    "translator = ctranslate2.Translator(\"enlt_ctranslate2\", device = \"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-tc-big-en-lt\")\n",
    "\n",
    "input_text = \"Hello, how are you?\"\n",
    "\n",
    "input_tokens = tokenizer.encode(input_text, return_tensors = \"pt\", add_special_tokens = True)\n",
    "input_tokens_str = tokenizer.convert_ids_to_tokens(input_tokens[0].tolist())\n",
    "\n",
    "results = translator.translate_batch([input_tokens_str], beam_size=1)\n",
    "output_tokens = results[0].hypotheses[0]\n",
    "\n",
    "output_text = tokenizer.decode(tokenizer.convert_tokens_to_ids(output_tokens))\n",
    "print(\"Translated text:\", output_text)\n",
    "\n",
    "!echo \"Labas, kaip sekasi?\" > wmt23_example-ref.en-lt.lt  # Reference translation\n",
    "!echo \"Sveiki, kaip esate?\" > wmt23_example-sys.en-lt.en  # System output\n",
    "!sacrebleu wmt23_example-ref.en-lt.lt -i wmt23_example-sys.en-lt.en -m bleu chrf ter\n",
    "\n",
    "!echo \"Hello, how are you?\" > wmt23_example.en-lt.en  # Source text\n",
    "!comet-score -s wmt23_example.en-lt.en -t wmt23_example-sys.en-lt.en -r wmt23_example-ref.en-lt.lt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "88qOPumiIHSy",
    "outputId": "788ff884-1d28-4b37-96e0-1f03f244780e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated dataset saved to translated_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Using my whole dataset - 1st Experiment\n",
    "\n",
    "dataset_path = \"train-00000-of-00001.csv\"\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "translator = ctranslate2.Translator(\"enlt_ctranslate2\", device=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-tc-big-en-lt\")\n",
    "\n",
    "def translate_text(input_text, src_lang, tgt_lang):\n",
    "    input_tokens = tokenizer.encode(input_text, return_tensors=\"pt\", add_special_tokens = True)\n",
    "    input_tokens_str = tokenizer.convert_ids_to_tokens(input_tokens[0].tolist())\n",
    "\n",
    "    results = translator.translate_batch([input_tokens_str], beam_size = 1)\n",
    "    output_tokens = results[0].hypotheses[0]\n",
    "\n",
    "    output_text = tokenizer.decode(tokenizer.convert_tokens_to_ids(output_tokens))\n",
    "    return output_text\n",
    "\n",
    "translated_sentences = []\n",
    "for index, row in data.iterrows():\n",
    "    source_text = row.iloc[2]  # English is in column 2\n",
    "    target_text = row.iloc[1]  # Lithuanian is in column 1\n",
    "\n",
    "    translated_text = translate_text(source_text, \"en\", \"lt\")\n",
    "    translated_sentences.append(translated_text)\n",
    "\n",
    "data['translated_text'] = translated_sentences\n",
    "\n",
    "output_path = \"translated_dataset_1.csv\"\n",
    "data.to_csv(output_path, index=False)\n",
    "print(f\"Translated dataset saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "3RWPuduO8QQN",
    "outputId": "193b8a61-7ab6-4e7c-8251-ac4d001fd551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 51.045566714196475\n",
      "CHRF score: 75.19829112934362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.3.5 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/unbabel_comet/wmt20-comet-da/checkpoints/model.ckpt`\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Predicting DataLoader 0: 100%|██████████| 66/66 [00:09<00:00,  6.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average COMET score: 0.702174056086326\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "from sacrebleu import corpus_bleu, corpus_chrf\n",
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "reference_texts = data[\"lt\"].tolist()\n",
    "system_outputs = data['translated_text'].tolist()\n",
    "\n",
    "# Compute BLEU score\n",
    "bleu_score = corpus_bleu(system_outputs, [reference_texts])\n",
    "print(f\"BLEU score: {bleu_score.score}\")\n",
    "\n",
    "# Compute CHRF score\n",
    "chrf_score = corpus_chrf(system_outputs, [reference_texts])\n",
    "print(f\"CHRF score: {chrf_score.score}\")\n",
    "\n",
    "# The COMET model\n",
    "model = download_model(\"wmt20-comet-da\")\n",
    "model = load_from_checkpoint(model)\n",
    "\n",
    "comet_inputs = [{\"src\": source, \"mt\": mt, \"ref\": ref}\n",
    "                for source, mt, ref in zip(data[\"en\"], system_outputs, reference_texts)]\n",
    "\n",
    "# Compute COMET scores\n",
    "comet_scores = model.predict(comet_inputs, batch_size=8)\n",
    "print(f\"Average COMET score: {sum(comet_scores.scores) / len(comet_scores.scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "4Cu24wko-CkW"
   },
   "outputs": [],
   "source": [
    "data['BLEU_Score'] = bleu_score.score\n",
    "data['CHRF_Score'] = chrf_score.score\n",
    "data['COMET_Score'] = comet_scores.scores\n",
    "\n",
    "data.to_csv(\"evaluation_results_1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "9jYV2X4HJayU",
    "outputId": "6ea7d8e0-ae30-47aa-ffe6-5a99bd5877e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches saved to batches_translated.csv\n"
     ]
    }
   ],
   "source": [
    "# Starting again (improving the model)\n",
    "\n",
    "dataset_path = \"train-00000-of-00001.csv\"\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Creating batches\n",
    "ranges = [\n",
    "    (1, 17), (18, 33), (34, 53), (54, 71), (72, 77), (78, 94), (95, 107),\n",
    "    (108, 128), (129, 139), (140, 148), (149, 160), (161, 171), (172, 184),\n",
    "    (185, 191), (192, 197), (198, 204), (205, 218), (219, 229), (230, 248),\n",
    "    (249, 261), (262, 273), (274, 283), (284, 294), (295, 306), (307, 315),\n",
    "    (316, 325), (326, 341), (342, 354), (355, 369), (370, 376), (377, 386),\n",
    "    (387, 397), (398, 409), (410, 419), (420, 431), (432, 441), (442, 458),\n",
    "    (459, 483), (484, 498), (499, 523)]\n",
    "\n",
    "batch_data = []\n",
    "\n",
    "for start, end in ranges:\n",
    "    batch = data[(data.iloc[:, 0] >= start) & (data.iloc[:, 0] <= end)]\n",
    "\n",
    "    batch_info = {'range': f\"{start}-{end}\",\n",
    "                  'en': batch.iloc[:, 1].tolist(),\n",
    "                  'lt': batch.iloc[:, 2].tolist()}\n",
    "    batch_data.append(batch_info)\n",
    "\n",
    "batch_df = pd.DataFrame(batch_data)\n",
    "\n",
    "final_data = []\n",
    "for i, batch in enumerate(batch_data, start = 1):\n",
    "    for en_sentence, lt_sentence in zip(batch['en'], batch['lt']):\n",
    "        final_data.append({'range': batch['range'],\n",
    "                           'en': en_sentence,\n",
    "                           'lt': lt_sentence})\n",
    "\n",
    "final_df = pd.DataFrame(final_data)\n",
    "\n",
    "output_path = \"batches_translated_2.csv\"\n",
    "final_df.to_csv(output_path, index = False)\n",
    "print(f\"Batches saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "2RvdkXo_hmtY",
    "outputId": "dd158a9f-7d4b-4a79-d9a5-9ebf73cf1896"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation complete and saved to 'translated_sentences_batches.csv'\n"
     ]
    }
   ],
   "source": [
    "# Translating again - 2nd Experiment\n",
    "\n",
    "batch_file_path = \"batches_translated.csv\"\n",
    "batch_data = pd.read_csv(batch_file_path)\n",
    "\n",
    "translator = ctranslate2.Translator(\"enlt_ctranslate2\", device=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-tc-big-en-lt\")\n",
    "\n",
    "def translate_batch(batch_texts):\n",
    "    if isinstance(batch_texts, pd.Series):\n",
    "        batch_texts = batch_texts.tolist()\n",
    "\n",
    "    tokenized_texts = [tokenizer.encode(text, return_tensors = \"pt\", add_special_tokens = True) for text in batch_texts]\n",
    "    tokenized_texts_str = [tokenizer.convert_ids_to_tokens(tokens[0].tolist()) for tokens in tokenized_texts]\n",
    "\n",
    "    results = translator.translate_batch(tokenized_texts_str, beam_size=1)\n",
    "    translated_texts = [result.hypotheses[0] for result in results]\n",
    "\n",
    "    translated_texts_decoded = []\n",
    "    for tokens in translated_texts:\n",
    "        decoded_text = tokenizer.decode(tokenizer.convert_tokens_to_ids(tokens), skip_special_tokens=True)\n",
    "        translated_texts_decoded.append(decoded_text)\n",
    "\n",
    "    return translated_texts_decoded\n",
    "\n",
    "translated_batches = []\n",
    "for _, batch in batch_data.iterrows():\n",
    "    sentence_range = batch['range']\n",
    "    english_sentences = batch['en']\n",
    "\n",
    "    if isinstance(english_sentences, str):\n",
    "        english_sentences = [english_sentences]\n",
    "\n",
    "    translated_sentences = translate_batch(english_sentences)\n",
    "\n",
    "    for en, lt in zip(english_sentences, translated_sentences):\n",
    "        translated_batches.append(list(batch) + [lt])\n",
    "\n",
    "translated_df = pd.DataFrame(translated_batches, columns=batch_data.columns.tolist() + ['translated_text'])\n",
    "translated_df.to_csv(\"translated_sentences_2.csv\", index=False)\n",
    "print(\"Translation complete and saved to 'translated_sentences_2.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "GDuY98MyxpKT",
    "outputId": "2d2ace0d-af77-4c06-99d3-110e0e9ea4ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 23.217024112151968\n",
      "CHRF score: chrF2 = 51.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.3.5 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/unbabel_comet/wmt20-comet-da/checkpoints/model.ckpt`\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Predicting DataLoader 0: 100%|██████████| 66/66 [00:08<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average COMET score: 0.678733026295416\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "reference_texts = translated_df[\"lt\"].tolist()\n",
    "system_outputs = translated_df['translated_text'].tolist()\n",
    "\n",
    "# Compute BLEU score\n",
    "bleu_score = corpus_bleu(system_outputs, [reference_texts])\n",
    "print(f\"BLEU score: {bleu_score.score}\")\n",
    "\n",
    "# Compute CHRF score\n",
    "chrf_score = corpus_chrf(system_outputs, [reference_texts])\n",
    "print(f\"CHRF score: {chrf_score}\")\n",
    "\n",
    "# The COMET model\n",
    "model = download_model(\"wmt20-comet-da\")\n",
    "model = load_from_checkpoint(model)\n",
    "\n",
    "comet_inputs = [{\"src\": source, \"mt\": mt, \"ref\": ref}\n",
    "                for source, mt, ref in zip(translated_df[\"en\"], system_outputs, reference_texts)]\n",
    "\n",
    "# Compute COMET scores\n",
    "comet_scores = model.predict(comet_inputs, batch_size=8)\n",
    "average_comet_score = sum(comet_scores.scores) / len(comet_scores.scores) if comet_scores.scores else None\n",
    "print(f\"Average COMET score: {average_comet_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "H2uE96VToI4c"
   },
   "outputs": [],
   "source": [
    "translated_df['BLEU_Score'] = bleu_score.score\n",
    "translated_df['CHRF_Score'] = chrf_score.score\n",
    "translated_df['COMET_Score'] = comet_scores.scores\n",
    "\n",
    "translated_df.to_csv(\"evaluation_results_2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "EvGS_nPJjERd",
    "outputId": "bb7e651e-f8a3-4ed2-a203-0cb25573a55d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated dataset saved to translated_dataset_2.csv\n"
     ]
    }
   ],
   "source": [
    "# ... Translating again - 3rd Experiment\n",
    "\n",
    "dataset_path = \"train-00000-of-00001.csv\"\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "translator = ctranslate2.Translator(\"enlt_ctranslate2\", device=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-tc-big-en-lt\")\n",
    "\n",
    "def translate_text(input_text, src_lang, tgt_lang):\n",
    "    input_text = input_text.lower()\n",
    "    input_tokens = tokenizer.encode(input_text, return_tensors=\"pt\", add_special_tokens = True)\n",
    "    input_tokens_str = tokenizer.convert_ids_to_tokens(input_tokens[0].tolist())\n",
    "\n",
    "    results = translator.translate_batch([input_tokens_str], beam_size = 1)\n",
    "    output_tokens = results[0].hypotheses[0]\n",
    "\n",
    "    output_text = tokenizer.decode(tokenizer.convert_tokens_to_ids(output_tokens))\n",
    "    return output_text\n",
    "\n",
    "translated_sentences = []\n",
    "for index, row in data.iterrows():\n",
    "    source_text = row.iloc[2]  # English is in column 2\n",
    "    target_text = row.iloc[1]  # Lithuanian is in column 1\n",
    "\n",
    "    translated_text = translate_text(source_text, \"en\", \"lt\")\n",
    "    translated_sentences.append(translated_text)\n",
    "\n",
    "data['translated_text'] = translated_sentences\n",
    "\n",
    "output_path = \"translated_dataset_3.csv\"\n",
    "data.to_csv(output_path, index=False)\n",
    "print(f\"Translated dataset saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "z2NZFmW2mG1f",
    "outputId": "2b8a544e-b538-4859-9178-525fd9da9feb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 15.575873330891495\n",
      "CHRF score: 57.73777628176294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.3.5 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/unbabel_comet/wmt20-comet-da/checkpoints/model.ckpt`\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Predicting DataLoader 0: 100%|██████████| 66/66 [00:08<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average COMET score: 0.32297127624442995\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "from sacrebleu import corpus_bleu, corpus_chrf\n",
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "reference_texts = data[\"lt\"].tolist()\n",
    "system_outputs = data['translated_text'].tolist()\n",
    "\n",
    "# Compute BLEU score\n",
    "bleu_score = corpus_bleu(system_outputs, [reference_texts])\n",
    "print(f\"BLEU score: {bleu_score.score}\")\n",
    "\n",
    "# Compute CHRF score\n",
    "chrf_score = corpus_chrf(system_outputs, [reference_texts])\n",
    "print(f\"CHRF score: {chrf_score.score}\")\n",
    "\n",
    "# The COMET model\n",
    "model = download_model(\"wmt20-comet-da\")\n",
    "model = load_from_checkpoint(model)\n",
    "\n",
    "comet_inputs = [{\"src\": source, \"mt\": mt, \"ref\": ref}\n",
    "                for source, mt, ref in zip(data[\"en\"], system_outputs, reference_texts)]\n",
    "\n",
    "# Compute COMET scores\n",
    "comet_scores = model.predict(comet_inputs, batch_size=8)\n",
    "print(f\"Average COMET score: {sum(comet_scores.scores) / len(comet_scores.scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "vchX44J7pP-9"
   },
   "outputs": [],
   "source": [
    "data['BLEU_Score'] = bleu_score.score\n",
    "data['CHRF_Score'] = chrf_score.score\n",
    "data['COMET_Score'] = comet_scores.scores\n",
    "\n",
    "data.to_csv(\"evaluation_results_3.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "boqG2G3rpl2j",
    "outputId": "6aff249e-492e-44f4-fad7-2ca4563c70b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated dataset saved to translated_dataset_3.csv\n"
     ]
    }
   ],
   "source": [
    "# ... Translating again and again... - 4th Experiment\n",
    "\n",
    "dataset_path = \"train-00000-of-00001.csv\"\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "translator = ctranslate2.Translator(\"enlt_ctranslate2\", device = \"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-tc-big-en-lt\")\n",
    "\n",
    "def translate_text(input_text, src_lang, tgt_lang):\n",
    "    input_tokens = tokenizer.encode(input_text, return_tensors = \"pt\", add_special_tokens = True)\n",
    "    input_tokens_str = tokenizer.convert_ids_to_tokens(input_tokens[0].tolist())\n",
    "\n",
    "    results = translator.translate_batch([input_tokens_str], beam_size = 5)\n",
    "    output_tokens = results[0].hypotheses[0]\n",
    "\n",
    "    output_text = tokenizer.decode(tokenizer.convert_tokens_to_ids(output_tokens))\n",
    "    return output_text\n",
    "\n",
    "translated_sentences = []\n",
    "for index, row in data.iterrows():\n",
    "    source_text = row.iloc[2]  # English is in column 2\n",
    "    target_text = row.iloc[1]  # Lithuanian is in column 1\n",
    "\n",
    "    translated_text = translate_text(source_text, \"en\", \"lt\")\n",
    "    translated_sentences.append(translated_text)\n",
    "\n",
    "data['translated_text'] = translated_sentences\n",
    "\n",
    "output_path = \"translated_dataset_4.csv\"\n",
    "data.to_csv(output_path, index=False)\n",
    "print(f\"Translated dataset saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "v_IyyTBiqDSG",
    "outputId": "dd31c3b0-70fc-4d22-838c-72c336b5166d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 58.45374263757147\n",
      "CHRF score: 80.04940936491896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.3.5 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/unbabel_comet/wmt20-comet-da/checkpoints/model.ckpt`\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Predicting DataLoader 0: 100%|██████████| 66/66 [00:08<00:00,  8.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average COMET score: 0.8725779490014735\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "from sacrebleu import corpus_bleu, corpus_chrf\n",
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "reference_texts = data[\"lt\"].tolist()\n",
    "system_outputs = data['translated_text'].tolist()\n",
    "\n",
    "# Compute BLEU score\n",
    "bleu_score = corpus_bleu(system_outputs, [reference_texts])\n",
    "print(f\"BLEU score: {bleu_score.score}\")\n",
    "\n",
    "# Compute CHRF score\n",
    "chrf_score = corpus_chrf(system_outputs, [reference_texts])\n",
    "print(f\"CHRF score: {chrf_score.score}\")\n",
    "\n",
    "# The COMET model\n",
    "model = download_model(\"wmt20-comet-da\")\n",
    "model = load_from_checkpoint(model)\n",
    "\n",
    "comet_inputs = [{\"src\": source, \"mt\": mt, \"ref\": ref}\n",
    "                for source, mt, ref in zip(data[\"en\"], system_outputs, reference_texts)]\n",
    "\n",
    "# Compute COMET scores\n",
    "comet_scores = model.predict(comet_inputs, batch_size=8)\n",
    "print(f\"Average COMET score: {sum(comet_scores.scores) / len(comet_scores.scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "jNGL0ViXqJaO"
   },
   "outputs": [],
   "source": [
    "data['BLEU_Score'] = bleu_score.score\n",
    "data['CHRF_Score'] = chrf_score.score\n",
    "data['COMET_Score'] = comet_scores.scores\n",
    "\n",
    "data.to_csv(\"evaluation_results_4.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
